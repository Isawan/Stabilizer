\section{Method}
\subsection{Camera model}

A pinhole camera model has been used.
We assume the surface being imaged is sufficiently far away from the camera.
This allows us to approximate any frame $t$ as an affine transformation of the previous frame $t-1$.
The model was restricted to a subset of affine transformations:
a counterclockwise rotation about $(0,0)$ by angle $\theta$,
a scaling by factor $s$ 
and spatial translations $\Delta x$ and $\Delta y$.
This improves the numerical stablity of the model by reducing the degree of freedom of the model to four.
The transformation can be expressed by a matrix product when the screen coordinates $(x,y)$ is augmented to $(x,y,1)$.
\begin{equation}
  \begin{bmatrix}
    x_{t}\\
    y_{t}\\
		1
  \end{bmatrix}
  =
  \begin{bmatrix}
		s\cos\theta & -s\sin\theta & \Delta x \\
    s\sin\theta &  s\cos\theta & \Delta y \\
    0           &  0           & 1
  \end{bmatrix}
  \begin{bmatrix}
		x_{t-1} \\
		y_{t-1} \\
		1
  \end{bmatrix}
\end{equation}


\subsection{Transformation estimation}

Transformation estimation involves finding an affine matrix that maps the coordinate of a frame $t$ to the coordinate space of the first frame.
Our algorithm is based on the feature point matching discussed by Kulkarni et. al. \cite{Kulkarni2017}.
An overview of the developed algorithm can be described by the following.

\begin{algorithmic}
	\FORALL{Frames}
    \STATE{Convert frame to greyscale}
		\STATE{Find frame's feature points and associated visual descriptor with ORB}
	\ENDFOR
	\FORALL{ Consecutive frames}
	  \STATE{Match the keypoints from previous frame}
		\STATE{Estimate the interframe transformation matrix $M_t$ with RANSAC}
    \STATE{Find the global transformation by $[ \prod^{t}_{i=0}M_i ]^{-1}$}
	\ENDFOR
\end{algorithmic}

For each frame we identify the feature point and descriptor with ORB.
ORB is a patent-free feature detector and descriptor based on FAST and BRIEF\cite{Rublee2011}.
Previous work shows comparable performance between ORB, SIFT and SURF\cite{Karami2017}.
ORB produces a binary visual descriptor,
the similarity metric is given by the Hamming distance.
We set the parameters such that a maximum of 500 features are extracted per frame.

We initially matched the keypoints in consecutive frames.
The Hamming distance is calculated between the binary descriptors.
The two nearest neighbours of each point is found by a brute force algorithm.
The ratio test proposed by D. Lowe\cite{Lowe2004} is applied to reduce false matches;
if the ratio of the two nearest neighbours is greater than a 0.8 then reject both matchings to that point.
Of the remaining matchings, we find the spatial position vectors from the frame $t-1$ keypoints to the frame $t$ keypoints.

The remaining matches still contains many false results.
The interframe transformation matrix is estimated using the RANSAC method\cite{Fischler1981}.
RANSAC is an iterative method to estimate a best fit model of data with significant outliers.
The estimation works provided that at least half of the matches are true matches.
The result of this is a matrix $M_t$ that transforms the previous frame $t-1$ into the frame $t$.

A global transformation must be calculated.
The global transformation describes the coordinate mapping of frame $t$ to the first frame.
This is represented by an affine matrix $M^G_t$.
The global transformation matrix is related to the interframe transformations by
\begin{equation}
	M^G_t =  \prod^{t}_{i=0}[ M_i ]^{-1}
\end{equation}
A performance improvement is gained by writing the recursive definition
\begin{equation}
  M^G_t = M^G_{t-1}[M_t]^{-1}
\end{equation}
where we set $M^G_0$ as the identity matrix and $M^G_{t-1}$ the global transformation of the previous frame.

Finally, we perform a coordinate transform to map each frame to the initial coordinates.
This is computed with a backward linear interpolation scheme. 



\subsection{Visualisation}

To visualise the stablisation, we developed two visualisation algorithms.

\subsubsection{Inplace frame stiching}


\subsubsection{Mache stiching}

Typically, image stitching algorithms find the minimum the number of frames to cover an area.
This reduces the boundary of frames and therefore reduces visual artifacts.
However, our method has unavoidable cumulative distortions over time;
infrequent frame stitching produces very noticeable artifacts at the boundaries.
Minimising the frames in this way loses the camera path information which is useful for analysis.
Alternatively, large number of stitchings is clearly not ideal due to many distorted boundary lines.

Mache stitching is a simple method to compromise between the two that we developed.
The algorithm has a threshold that controls the frequency of frame stitches.
The threshold $r$ is bounded by $0\leq r \leq1$, a higher value results in more stitches.

\begin{algorithmic}
  \STATE{Start with a blank canvas}
  \STATE{Paint initial frame onto the canvas}
  \STATE{Record the position of the corners of the initial frame}
  \FORALL{Frame}
    \STATE{Transform corners using $M^G_t$ into global coordinates}
    \STATE{Calculate intersection between frame and last painted frame}
    \STATE{Calculate Ratio of intersecting area to frame area}
    \IF{$Ratio > Threshold$}
      \STATE{Warp frame to global coordinates}
      \COMMENT{This removes black banding at the edge due to interpolation}
      \STATE{Trim the edges by erosion with a $5\times5$ square structuring element}
      \STATE{Paint frame onto the canvas}
      \STATE{Record the position of the corners of the frame}
    \ENDIF
  \ENDFOR
\end{algorithmic}

An initial issue with both implementation is memory usage due to the high resolutions of the final image.
It is impractical to handle all stabilized frames in memory.
This issue was resolved by modifying the algorithm to be lazy evaluated.

\subsection{Evaluation}

We tested our developed algorithm in three stages.
First, we generated a video tracking across a surface of noise to test the ideal case.
A background was generated by an edge detected Perlin noise \cite{Perlin1985} with a color correction.
This noise was chosen as it has distinctive corner features for the algorithm to track.
A video was produced where the video tracked a $250\times250$ window along the line parameterized by
\begin{align}
  y & = 50 + 50\sin{\left(\frac{t}{50}\right)} + t \\
	x & = t 
\end{align}
Small random jitters scale and translate the window was also applied.
When saving the video, we upscaled the window to $500X500$ to reduce dataloss through video encoding.
This path was chosen as it is simple and asymmetric under reflections of the axes;
the path is asymmetric to avoid errors 'cancelling' out when moving up and down.
Two points were chosen, one in the initial frame and one in the final frame.
The distance between the points were calculated in the background image.
We applied the algorithm to the video and measured the distance based on the panorama image. 
The distance of the panorama image was divided by two to correct for the upscaling.

A tracking shot of a measuring tape was taken to test the accuracy of the algorithm.
The accuracy of the algorithm in practice was tested by comparison of estimated spatial distance between known separations.
A measuring tape was extended on a table surface.
Miscellaneous items were placed around the measuring tape to provide the algorithm with feature points to track.
A camera was positioned above the tape such that a small region of the measuring tape was visible.
The camera was tracked along the measuring tape with small motions orthogonal to the tracking direction.
Our algorithm was applied to the recorded video to produce a panorama.
Two points were picked along the tape.
The true distance between the ticks is found by measuring the ticks.
The pixel to length ratio was found by measurement of the pixel length between two fixed point on the tape in a single frame.
The video distance is then compared to the real distance.

UAV footage taken over the University of Nottingham School of Physics and Astronomy was provided for our testing.
UAV footage was stabilised and visually compared with an aerial image.
Bing maps was used as a reference as it provided the latest image of the area.
We were unable to find a recent aerial image that included the renovations to George Green Library.
