\section{Discussion}
\subsection{Simulated data}
The simulated data was used to test the correctness of the in-frame stitching algorithm,
the result of which is shown in figure \ref{fig:simnoise_a}.
We observe that when a frame moves out of an area,
there is a chance that it would leave behind a line of dark pixels, rather than the image which should be displayed.
This is because when transformations are applied to the frames, a small number gain a dark, but not completely black, border due to interpolation when the transformation is applied.
A problem arises due to the stitching algorithm assuming that only areas with a brightness of 0 are outside of all image data, 
meaning these erroneous near-black strips are not replaced by subsequent frames and therefore remain in the final image. 
We resolve this problem with mache stitching where the border is eroded in figure \ref{fig:simnoise}.

%The 3\% error on the distance is important has it allows the development o

\subsection{Measuring tape}

As seen in figure \ref{fig:expect},
the software was able to correctly maintain the scale of a recorded object but only up to a certain point.
All points below $50 cm$ are scaled correctly
yet there seems to be some minor discontinuities before $60cm$ is reached.
Figure \ref{fig:measure} shows these discontinuities clearly.
Near the right-hand side of the image there is a point where the image of the tape becomes disconnected with itself,
resulting in a difference of angle in the right-most frames compared to the rest of the image.
The discontinuity is caused by a vertical gap in which very little features are available except for the measuring tape,
resulting in the ORB algorithm finding few feature points outside of the tape.
This is a problem as most of these remaining points are situated on the identical ticks on the tape's scale,
making it difficult to accurately match each point to itself in neighbouring frames.
It is therefore important to ensure that there are sufficient objects which exist in both frames before attempting to calculate transformations,  otherwise artefacts are highly likely.

\subsection{Drone footage}

Finally, the software was tested using real-world drone footage.
The video consisted of a tracking shot which starts out over a car park, then moves over a building.
Initially, as seen in the lower section of figure 5a, the process works nearly as intended,
with only minor artefacts.
After passing over a building, however, major inconsistencies start to occur.
The most obvious case is the area above the courtyard in the centre of the image,
where we see a copy of the courtyard and the buildings surrounding it.
Further inconsistencies may be spotted by using real satellite imagery of the area, provided in figure 5b.
Of particular note is that although many buildings in the upper section have incorrect locations and scales,
their rotations seem mostly unaffected.
It appears that each time the drone moves over a building, the displayed frame size is reduced.
This is most likely due to the changing size of the buildings' walls due to parallax, and shows that in this case our camera model is flawed.
The effect of the shrink is small when flying over the car park which shows that the algorithm will work as long as surface is low-laying.
